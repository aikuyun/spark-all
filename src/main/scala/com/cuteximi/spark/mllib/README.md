## 线性回归

回归是统计学中最有力的工具之一。机器学习监督学习算法分为分类算法和回归算法两种，其实就是根据类别标签分布类型为离散型、连续性而定义的。回归算法用于连续型分布预测，针对的是数值型的样本，使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签。

线性回归图解，什么是线性回归，原理是什么？什么是梯度下降算法。

![](https://github.com/aikuyun/spark-all/blob/master/src/image/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png)


回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。那么什么是线性关系和非线性关系？

比如房价和面积，存在一种关系。房价可以看做一系列的散列点，如果用一条直线来描述这个关系，那就叫线性回归。如果使用一条曲线来描述这个关系，那就是非线性回归分析。

回归目的之一是 预测, 预测的话只能降低误差。

## 几个概念：

建模：选出合适的算法，利用那个历史数据求出算法的具体参数

训练集: 参与算法的计算的数据

测试集: 参数测试的数据

拟合函数 y = ax + b

损失函数：在统计学中损失函数是一种衡量损失和错误（这种损失与“错误地”估计有关，如费用或者设备的损失）程度的函数。